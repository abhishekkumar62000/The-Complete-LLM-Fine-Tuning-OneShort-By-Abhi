{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fstTqT2KYIoK"
   },
   "source": [
    "Where BERT is Still Used Today\n",
    "\n",
    "1. Search & Retrieval (Vector Search, RAG Base Models)\n",
    "\n",
    "- For generating dense embeddings (e.g., Sentence-Transformers, MiniLM).\n",
    "\n",
    "- Stored in FAISS, Pinecone, Milvus for fast similarity search.\n",
    "\n",
    "- Used in smaller LLM pipelines (retriever + generator architecture).\n",
    "\n",
    "2. Enterprise-level NLP Tasks (Fast & Cost-Effective)\n",
    "\n",
    "- Named Entity Recognition (NER)\n",
    "\n",
    "- Sentiment Analysis\n",
    "\n",
    "- Classification tasks (spam detection, intent classification)\n",
    "\n",
    "- Summarization using lightweight variants (DistilBERT).\n",
    "\n",
    "3. Hybrid Pipelines with LLMs\n",
    "\n",
    "- BERT embeddings for the retriever, then an LLM generates the answer (RAG architecture).\n",
    "\n",
    "4. Multilingual NLP\n",
    "\n",
    "- XLM-R (a multilingual BERT version) is still a top choice for 100+ languages.\n",
    "\n",
    "- Used for translation and cross-lingual search.\n",
    "\n",
    "5. On-Device / Low-Latency Inference\n",
    "\n",
    "- For mobile apps and edge devices where GPT/Claude can’t run.\n",
    "\n",
    "- Quantized DistilBERT/MiniLM models for chatbots and offline NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWc1nGddZExy"
   },
   "source": [
    "| **Model**           | **Main Use-Cases**                                  | **Why Still Used**                    |\n",
    "| ------------------- | --------------------------------------------------- | ------------------------------------- |\n",
    "| BERT / DistilBERT   | NER, classification, embeddings                     | Small, fast, cheap inference          |\n",
    "| RoBERTa / DeBERTa   | Classification, QA, summarization                   | High accuracy, Kaggle/enterprise use  |\n",
    "| MPNet / MiniLM      | Vector search, semantic retrieval (RAG)             | Best for FAISS/Pinecone retrieval     |\n",
    "| T5 / Flan-T5        | Summarization, translation, instruction tasks       | Lightweight text-to-text generation   |\n",
    "| BART / Pegasus      | Abstractive summarization                           | Less resource-hungry than LLMs        |\n",
    "| XLNet / Electra     | Classification, QA (legacy setups)                  | Still optimized for speed             |\n",
    "| XLM-R / mT5 / LaBSE | Multilingual NLP, translation, cross-lingual search | 100+ language support, enterprise use |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sD6ApJatYMS8",
    "outputId": "fa74e957-43df-4516-db37-736e792aa000"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets fsspec transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j46yH4p_zZmo"
   },
   "source": [
    "# First Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfUAIalozeSq"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SZQEPvczeVs"
   },
   "outputs": [],
   "source": [
    "# take the below complex dataset\n",
    "# load_dataset(\"ag_news\")\n",
    "# load_dataset(\"dbpedia_14\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm34eDTkzeYK"
   },
   "outputs": [],
   "source": [
    "# Customer feedback classification (positive/negative/neutral)\n",
    "# Support ticket intent detection (billing, technical, general)\n",
    "# Email/topic categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "8132c0699db843129fa3db5b6a9a9d84",
      "2f259433eec3443084e309574f7cb687",
      "6b11d46bacf348f9b8b6135d81e0ef62",
      "3f423ee439a74e30b1aa5d76a5fb2b4d",
      "b35710cdcf4248ffa729b04887026cb6",
      "40412a11abab4aa7890761eaa2972937",
      "c3785e98994a4d4e9190a0745c11a8b5",
      "895abad4ce96469eb5fbebc13eb21604",
      "ea5e7750dce94f48a941976325ac0655",
      "f5d399904e0f49b1a345ee96066c9885",
      "38033995e8494e8a91aa849f773d410f",
      "88e7c8bc82794b858148f3ce87233b53",
      "bf0773174a3848769efe386194d0aa5a",
      "2fc5ed8932e64b7a8800f2f51c99779a",
      "dcaa298eb4114512b48ae078b0296329",
      "2c07129ad48343aeaad13f95dca6b6eb",
      "c3a615dffb7b4f2893dd48233cac977e",
      "2b649a13854f46cdba78744b9a08ef33",
      "ed2167c8ed3846439fd71fca2acf0bae",
      "4fb5e0275b4e419cb9f4ac69960d55fe",
      "29cdb909a11548a8a3511735592b2735",
      "e98ec7dee665400cbda567ecbe52578c",
      "d7391c7f11044e1bb0e1013b2d69c595",
      "c3f1a1aad3db4f7a95b7ac710a23bf88",
      "79dafb63ee734dfaa95dbbfb9dc6a4d5",
      "9d9af211930c42e6aae6441d2fc59b36",
      "10936d30901b4d6092dc82a7dc99e4ca",
      "3e6631cf7174411c975076fbe17a6057",
      "6ee2d5fd5a3d412882faf7cb3f665d22",
      "8b9437286da14c4396f469b9c5c04531",
      "360a8ce71467443094b0f1fbcd0e4d85",
      "5f3d123b85444b969ed2544bc615527c",
      "b113ee1c140542e296c664d79e8a2f64",
      "9afc2b48076043e781a13e6397b5f6e9",
      "675d4aaa45724a30806ee2c5a14279f7",
      "72aafd0084cb4c43b8a5a94e80e22bd7",
      "42468e7b35d64ab0b5d7b23aa1c92c23",
      "6f881b824c704b03850500cd34f1bef6",
      "7754b3c342ed432b91354e4e1e52f307",
      "5324576472214c79a16ca87bfc39f00c",
      "88aa9d88293e46b783002730e15dca7a",
      "07b2b19bec2342589dff49062250306e",
      "dc47b78df0bb4ebc91900e261edfcf7a",
      "478cdf954b754850b684fccd56cf29aa",
      "dfb37bacea964e128792ab01bb8d5657",
      "4ca70806bf834b189c7b283b43b6d760",
      "c0351b84db70466ba02dd838cba0c8e0",
      "4ce8e755e54d4344b5e946be96bd9035",
      "75eb72285aee45bc850cd8ad6810ff49",
      "1760a2933b8244a6bd3673109b7f1fba",
      "91bbdf475af14a378e6eae581392013f",
      "2a95d24126ab4a239bd222fcbd744579",
      "91574689901e428e852186875745bfc0",
      "f1486e3ee4d14fc0bc9b14f41d96e3fd",
      "9670992aca8749aa9254a5061d217cc3",
      "43491cb9863e4d2f9ee516e3b5cd0583",
      "eebdb7319c5f47e0a4ab10ecf0d9ff10",
      "c4ba3763c1e14e8cab8cdcbf116249bc",
      "cfb3f69581f946dcb15401d38dcf9581",
      "e758c5cb2ae9444eb2a56eecc13a5215",
      "179a700945d3465fa7027c02822db892",
      "6a1a0d77d9a34067b789980de4d6a089",
      "84edc45a7910423e9adb070f4518ae51",
      "8e19718377364a01a1a9425c17ce83ca",
      "b757c1b9ea2a4dcfbf5bf639533540b8",
      "1d7875ca5f8e4ebaac85faab21ce0d55",
      "f341263dd8cc4a2394f7497cdf18990a",
      "1e9ed994c04d477f85a8f27de1827ab1",
      "05829f7a39394259bb5ef413f11d2794",
      "32f46bccf8af40a5b32f18d40a24e35b",
      "b25af2089c17465db01a365664e8b768",
      "a9765ceb350b4d66abf690c12ef3486f",
      "6b608a558b734d4d93d2e2f37daf9111",
      "47b5fbf9a54e48fbb84bb729e3359dfc",
      "6ef0ab4ffce74f619e26ceb552194830",
      "d2ed7545af8a48afb6f832858f140414",
      "1ff5073d617042508bfb01801e87803c"
     ]
    },
    "id": "7imbEzlkzecj",
    "outputId": "b4e238b1-0a8f-4a3e-ac7b-868c115f9b0a"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "# Load IMDB dataset and subset\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jUPG3wQtLx2E",
    "outputId": "231c0f85-5a1f-4459-db2b-7c38047d564c"
   },
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJkLvHt1zee9"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"].select(range(1000))\n",
    "test_dataset = dataset[\"test\"].select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkEOxqA2z21_"
   },
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkLNandqV-UD"
   },
   "source": [
    "\n",
    "\n",
    "##### The raw text column (Each row in the IMDB dataset contains a review text)\n",
    "##### It will pad each sentence to the same length (up to 256 tokens)\n",
    "##### If the text is longer than 256 tokens, it will truncate (cut) it\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ijD42dn2z24i"
   },
   "outputs": [],
   "source": [
    "# Tokenization function\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YaRZULDgz27F"
   },
   "outputs": [],
   "source": [
    "# Apply tokenization + rename + format in a single flow\n",
    "def preprocess(ds):\n",
    "    ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])  # remove raw text (saves memory)\n",
    "    ds = ds.rename_column(\"label\", \"labels\")\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b9a0bf2b685c456094b6c68c58dd548c",
      "f6484d63485841629b61f5c5322de600",
      "c69535ecf5b44a0793aa89076b1dad92",
      "ac199b4953b94ad9a49e7824ffd7d7db",
      "8158c78b8f364a11aa81cfde354adcd5",
      "d88fd75783ea4f1aaf140c82ab7d0109",
      "f7abcb7aa80643d5bbf34929f091b984",
      "eb00ee1512744800a53dd51f0cdb6bf6",
      "fe2f4970245049b9a115439e3ce62215",
      "f598e869df6043b397d666edef84b83b",
      "21e9fd984ada4cb7919d9bb8456ad3a9"
     ]
    },
    "id": "-eBrtHUcz29W",
    "outputId": "c41fcdbb-c075-4965-993f-f77a0e71f99d"
   },
   "outputs": [],
   "source": [
    "train_dataset = preprocess(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "df5f5e08bae14d359a4d8dfa8818dedd",
      "12eb8666f24c491c940eb91f3ed7704e",
      "470f347202ea4b9f9a481e63136a50d8",
      "e1fab53d117a41a990bc2a218884a456",
      "3647125eb3c44af2930596d8234fcb6f",
      "d8e86dca159a4bc8bc53d8958d73636c",
      "6fef6a77c9e74bf09d4a555ce95f69c6",
      "896aec85c08548d29846fe7ae6fb8760",
      "edea57d9bf7342cca920a9788ccec426",
      "4f32bd0d6be44f3985aa6d91bb9b6e72",
      "346a937a5ab4492db890c43eea343f74"
     ]
    },
    "id": "vFKiKAIiz-GC",
    "outputId": "630fcf34-fc24-41e1-a315-587265d6705a"
   },
   "outputs": [],
   "source": [
    "test_dataset = preprocess(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105,
     "referenced_widgets": [
      "a20a46e945034fbdad9a656e42b14cef",
      "7643f8fdf24b4b1da31927015b7f5b5c",
      "c0d71f4ae4c34e86a7484985a201d485",
      "a5041c6bce6541be881c86e7aa9aa654",
      "450961029e444a779af85584954a7f5b",
      "fc40288439884d6c8ac92a83c344288b",
      "9f5e8d5425cc4f77b608439d14cad739",
      "806b6218761b462383901f85bd7bfa1b",
      "6ec92b211316487b957f3de96e36f629",
      "d9135f99ac074e3cb91893319bcfc8b4",
      "39b271c5507d418fa2ef8264383f8c36"
     ]
    },
    "id": "ERFLeSvwz-8P",
    "outputId": "72f06084-d945-4eaa-ddfe-4f3d8ee8f184"
   },
   "outputs": [],
   "source": [
    "# 3. Model load\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ml-Y00kfRxNj",
    "outputId": "ea565cf3-6710-4e99-820a-42413f47aefc"
   },
   "outputs": [],
   "source": [
    "for layer in model.bert.encoder.layer:\n",
    "  print(layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tldMs-8z--1"
   },
   "outputs": [],
   "source": [
    "# #Classifier head trainable rahega by default\n",
    "\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False  # Freeze BERT encoder\n",
    "\n",
    "# #Unfreeze last 2 encoder layers\n",
    "\n",
    "# for layer in model.bert.encoder.layer[-2:]:\n",
    "#     for param in layer.parameters():\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JH89dQ2FsI8B"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-finetuned-imdb\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cR8gys70Yi6"
   },
   "outputs": [],
   "source": [
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "cQhrExLJ0bpQ",
    "outputId": "d3a778de-e9be-424c-95b7-741f98817bbb"
   },
   "outputs": [],
   "source": [
    "# 6. Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0zDMVGrrqg-",
    "outputId": "03ba6dd9-a0f7-454a-afc0-b8b9ec483bb9"
   },
   "outputs": [],
   "source": [
    "!tensorboard --logdir=./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBmgg3Pn0dMr"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aLbbW4x0dPV",
    "outputId": "08b8d80e-0a42-40e5-be16-7fd8e87920f3"
   },
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "c2aJKe_h0e8j",
    "outputId": "9ca9a349-ae6f-422a-b5d9-4c5856cf3431"
   },
   "outputs": [],
   "source": [
    "# 7. Evaluate\n",
    "metrics = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Y3Wug-n0fAk",
    "outputId": "07f574f5-0ddd-4ad1-8ba8-91ed8228617d"
   },
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmJq0Nwz0hs3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsQZuD860jps"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"/content/bert-finetuned-imdb\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"/content/bert-finetuned-imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N5UF9Zzh0jsN",
    "outputId": "9417774d-08e1-4f04-b481-f32cdc250024"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k-5cIQ3a0mjp"
   },
   "outputs": [],
   "source": [
    "# Predict\n",
    "text = \"This movie was amazing and I loved the acting!\"\n",
    "result = classifier(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n1Q9PAln0mmC",
    "outputId": "d271d901-4afb-4e30-d018-011818ec7085"
   },
   "outputs": [],
   "source": [
    "print(result)  # Example: [{'label': 'POSITIVE', 'score': 0.98}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdev1wUG0q2h"
   },
   "source": [
    "### Pushing it to Huggingfacehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "0bcad6973f1d4c1998efc2ec965eade0",
      "72d30933ca7246098be7d85b4b18a1c1"
     ]
    },
    "id": "kLeh1DNI0qOA",
    "outputId": "aeb9a665-00ba-4b98-f3ac-c396c5c47ad2"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0iDLXdrf0qQd",
    "outputId": "822fa86a-9dd8-4f8c-caac-edb81344010e"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(whoami())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "QblwcgII0vUC",
    "outputId": "c48ac480-e69a-4ddb-e826-5f83f4e4b98b"
   },
   "outputs": [],
   "source": [
    "tokenizer.push_to_hub(\"sunny199/my-bert-imdb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "94f074ca5147474882a2ac4e7c87bee4",
      "c890938fdfd74401a40e19c94016a94b",
      "5876f07e1a864121a2b882721a27fac4",
      "bdc97a6e4fb64e4c94724b6016a3631f",
      "d62773dbf63642beaa70f00030ecbcc7",
      "0aede7cf12aa445996476fb4abb492f0",
      "32875aaaf9f748b8aeafe2b501db9c61",
      "9c98f7a3ea7f482cbaab7db5163c330b",
      "841edb9cbb64409b9c650b81f5c33a07",
      "205b6a57ad564602bfd39f5fc0c154bc",
      "1a6172a4f5784bebbee84bb165097420",
      "d0a551b8c51e4886851b3d88ef2d5b2f",
      "d10f35cfc4354a06a4bc288a7c8796ea",
      "e91eb3b17a3b41fab5ca76301a0aed2a",
      "f7b09ee396804cf49e6c690e099cfb9e",
      "ecd068ad899043c1ac255c5e5ffb2b57",
      "d0e3e27d38d84d9aac6b757a98568739",
      "28d22ee4e8af4e14adf5ccb87056f465",
      "11fb2a9d4c914f92ac216951a90fba0d",
      "3a9087baa4424a18a51d4903f83177a4",
      "d1791986dd844069b3f01f552a1beb4d",
      "d3f2f05a5fb94073b4e54aeff290baa8",
      "3e01ad5a584d4b149cd2cef1a690c29e",
      "df43abfc904e4629bfb9060529875f32",
      "5a6f81626a364444818dd5c5aabd6931",
      "d859a557e5724a55884ff42f371d023f",
      "6e8a4e1ded1c4da7ad042e3784dd9ef3",
      "ca2912becc2c48ff858229ec8c91b9b4",
      "5581017a112642df92212d55c33319dd",
      "d52a01f31aa44fffae5016ee8f22a8e5",
      "82ba3bb2ae7a40db837fe6b5d9bf9090",
      "914f2a183a594f528afff62ee1a087db",
      "8c746cbb14d04b5bb7ad1cfd7af6a916"
     ]
    },
    "id": "NP7qOhFE0x54",
    "outputId": "15350539-a6cb-4f5e-f631-f7a3c930231d"
   },
   "outputs": [],
   "source": [
    "trainer.push_to_hub(\"sunny199/my-bert-imdb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjp2zHAwW3dN"
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# # 1. Load IMDB dataset (subset for speed)\n",
    "# dataset = load_dataset(\"imdb\")\n",
    "# train_dataset = dataset[\"train\"].select(range(1000))\n",
    "# test_dataset = dataset[\"test\"].select(range(500))\n",
    "\n",
    "# # 2. Initialize tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # 3. Tokenization function (no fixed padding here)\n",
    "# def tokenize_fn(examples):\n",
    "#     return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "# # 4. Preprocess dataset (map + rename + torch format)\n",
    "# def preprocess(ds):\n",
    "#     ds = ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])  # remove raw text\n",
    "#     ds = ds.rename_column(\"label\", \"labels\")\n",
    "#     ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "#     return ds\n",
    "\n",
    "# train_dataset = preprocess(train_dataset)\n",
    "# test_dataset = preprocess(test_dataset)\n",
    "\n",
    "# # 5. Initialize model\n",
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# # 6. Data collator (dynamic padding)\n",
    "# data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# # 7. Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./bert-finetuned-imdb\",\n",
    "#     num_train_epochs=1,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     logging_dir=\"./logs\",\n",
    "#     logging_steps=50,\n",
    "#     learning_rate=2e-5,\n",
    "#     weight_decay=0.01,\n",
    "#     eval_steps=500,\n",
    "#     save_steps=500,\n",
    "#     save_total_limit=1,\n",
    "#     report_to=\"none\"\n",
    "# )\n",
    "\n",
    "# # 8. Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=test_dataset,\n",
    "#     data_collator=data_collator,  # dynamic padding here\n",
    "# )\n",
    "\n",
    "# # 9. Train\n",
    "# trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3WGIzCWzXC7"
   },
   "source": [
    "# Finetune on multiple problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6KouthQeTBiw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    BertForTokenClassification,\n",
    "    BertForQuestionAnswering,\n",
    "    get_linear_schedule_with_warmup ###### Gradually warms up then decays learning rate for stable BERT training.\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.optim import AdamW #Adam Optimizer with Weight Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "isHzO0jScVsk",
    "outputId": "f3f1ff2a-74f7-4730-c0dc-48c319b3e31c"
   },
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpBWBTXxM8jF"
   },
   "source": [
    "Dataset size = 800 samples\n",
    "\n",
    "batch_size=8 → 1 epoch = 800/8 = 100 batches\n",
    "\n",
    "epochs=3\n",
    "\n",
    "To:\n",
    "\n",
    "total_steps = 100 × 3 = 300\n",
    "\n",
    "300 total optimizer updates honge.\n",
    "\n",
    "Agar warmup 10% hai → 30 steps warmup, baaki 270 steps me LR linearly decrease hoga."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wV6nHeacP3Rb"
   },
   "source": [
    "Hugging Face’s direct map() + set_format() approach (used in the IMDB example)\n",
    "\n",
    "Custom PyTorch Dataset class (TextClassificationDataset)\n",
    "\n",
    "when the below class will be used?\n",
    "\n",
    "You haven’t preprocessed the dataset into Hugging Face format (like map() + set_format()).\n",
    "\n",
    "You only have Python lists (train_texts, train_labels).\n",
    "\n",
    "You need custom preprocessing logic (e.g., a different tokenizer, extra transformations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0bmf-c5e3ll"
   },
   "outputs": [],
   "source": [
    "class Basket:\n",
    "    def __init__(self, fruits):\n",
    "        self.fruits = fruits  # List of fruits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fruits)  # Kitne fruits hai total\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.fruits[idx]  # Index se specific fruit nikalna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "647kUXEBfA3_"
   },
   "outputs": [],
   "source": [
    "basket = Basket([\"Apple\", \"Banana\", \"Mango\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p7WaIbsge4f4",
    "outputId": "52a37842-8604-4e34-e55b-9c1d71bf62a3"
   },
   "outputs": [],
   "source": [
    "print(len(basket))\n",
    "print(basket[0])\n",
    "print(basket[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBhaZgqPTDWm"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KL10SQ9BefoH"
   },
   "source": [
    "1. Initialize the classifier – load BERT model, tokenizer, and move to device.\n",
    "\n",
    "2. Load IMDb data – sample train and test texts with labels.\n",
    "\n",
    "3. Train the model –\n",
    "\n",
    "- Convert texts and labels into a dataset.\n",
    "\n",
    "- Use DataLoader for batching.\n",
    "\n",
    "- For each epoch and batch: forward pass, compute loss, backward pass, update weights, adjust learning rate.\n",
    "\n",
    "4. Evaluate the model –\n",
    "\n",
    "- Run on test data without gradients.\n",
    "\n",
    "- Collect predictions and compute accuracy, F1, and report.\n",
    "\n",
    "5. Predict new texts –\n",
    "\n",
    "- Tokenize, run through the model, apply softmax, and return predictions with probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StaWtdUXTHPj"
   },
   "outputs": [],
   "source": [
    "# BERT Text Classifier\n",
    "class BERTTextClassifier:\n",
    "    \"\"\"BERT for Text Classification (Sentiment, Spam etc.)\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', num_classes=2, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name, num_labels=num_classes\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_imdb_data(self, sample_size=5000):\n",
    "\n",
    "        \"\"\"Load IMDb movie reviews dataset\"\"\"\n",
    "\n",
    "        print(\"Loading IMDb dataset...\")\n",
    "\n",
    "        dataset = load_dataset(\"imdb\")\n",
    "\n",
    "        # Sample data for faster training\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                       min(sample_size, len(dataset['train'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                      min(sample_size//4, len(dataset['test'])),\n",
    "                                      replace=False)\n",
    "\n",
    "        # Convert numpy.int64 → int for indexing\n",
    "        train_texts = [dataset['train'][int(i)]['text'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['label'] for i in train_indices]\n",
    "\n",
    "        test_texts = [dataset['test'][int(i)]['text'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['label'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_texts)}\")\n",
    "        print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "        return train_texts, train_labels, test_texts, test_labels\n",
    "\n",
    "    def train(self, train_texts, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "\n",
    "        \"\"\"Train the text classifier\"\"\"\n",
    "\n",
    "        train_dataset = TextClassificationDataset(\n",
    "            train_texts, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "        total_steps = len(train_loader) * epochs\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def evaluate(self, test_texts, test_labels, batch_size=8):\n",
    "\n",
    "        \"\"\"Evaluate the text classifier\"\"\"\n",
    "\n",
    "        test_dataset = TextClassificationDataset(\n",
    "            test_texts, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "        report = classification_report(true_labels, predictions,\n",
    "                                     target_names=['Negative', 'Positive'])\n",
    "\n",
    "        return accuracy, f1, report\n",
    "\n",
    "    def predict(self, texts):\n",
    "\n",
    "        \"\"\"Predict sentiment for new texts\"\"\"\n",
    "        predictions = []\n",
    "        probabilities = []\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        for text in texts:\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "                pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "\n",
    "                predictions.append(pred)\n",
    "                probabilities.append(probs)\n",
    "\n",
    "        return predictions, probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZF9kCF-ETQ0"
   },
   "source": [
    "Tokens: [\"John\", \"lives\", \"in\", \"London\"]\n",
    "\n",
    "Labels: [1, 0, 0, 2]  \n",
    "\n",
    "1 = B-PER (John is a person)\n",
    "\n",
    "0 = O (Outside entity)\n",
    "\n",
    "2 = B-LOC (London is a location)\n",
    "\n",
    "Step 1: Tokenizer output\n",
    "\n",
    "Original words: John | lives | in | London\n",
    "\n",
    "BERT tokens:    [CLS], John, lives, in, Lon, ##don, [SEP], [PAD]...\n",
    "\n",
    "Word IDs:       None,   0,    1,    2,   3,    3,    None, None...\n",
    "\n",
    "Step 2: Label Alignment\n",
    "\n",
    "Input IDs:     [101, 1001, 2002, 1999, 3001, 3010, 102, 0, 0, 0]\n",
    "\n",
    "Word IDs:      [None,   0,   1,   2,   3,   3,  None, None, None, None]\n",
    "\n",
    "Aligned Labels:[-100,   1,   0,   0,   2, -100, -100, -100, -100, -100]\n",
    "\n",
    "Step 3: Final output model ko ye milega\n",
    "\n",
    "{\n",
    "  'input_ids': tensor([...]),         # token IDs\n",
    "\n",
    "  'attention_mask': tensor([1,1,1,...]),  # 1 for real tokens, 0 for pads\n",
    "\n",
    "  'labels': tensor([-100,1,0,0,2,-100,-100,...])  # aligned labels\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfvPhWd5Tz2s"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NERDataset(Dataset):\n",
    "    \"\"\"Dataset for Named Entity Recognition\"\"\"\n",
    "\n",
    "    def __init__(self, tokens_list, labels_list, tokenizer, max_length=512):\n",
    "        self.tokens_list = tokens_list\n",
    "        self.labels_list = labels_list\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokens_list[idx]\n",
    "        labels = self.labels_list[idx]\n",
    "\n",
    "        # Tokenize with word-level alignment\n",
    "        encoding = self.tokenizer(\n",
    "            tokens,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            is_split_into_words=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Get word alignment for batch index 0\n",
    "        word_ids = encoding.word_ids(batch_index=0)\n",
    "        aligned_labels = []\n",
    "        previous_word_idx = None\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned_labels.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned_labels.append(labels[word_idx] if word_idx < len(labels) else 0)\n",
    "            else:\n",
    "                aligned_labels.append(-100)  # Ignore subword tokens\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # Ensure aligned_labels length matches max_length\n",
    "        if len(aligned_labels) < self.max_length:\n",
    "            aligned_labels += [-100] * (self.max_length - len(aligned_labels))\n",
    "        elif len(aligned_labels) > self.max_length:\n",
    "            aligned_labels = aligned_labels[:self.max_length]\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Shape: (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),  # Shape: (max_length,)\n",
    "            'labels': torch.tensor(aligned_labels, dtype=torch.long)  # Shape: (max_length,)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7NNL8w1sG8hX"
   },
   "source": [
    "| **Label**  | **Full Form**          | **Meaning (Hindi + English)**                                          | **Example**                                                       |\n",
    "| ---------- | ---------------------- | ---------------------------------------------------------------------- | ----------------------------------------------------------------- |\n",
    "| **O**      | Outside                | Koi entity nahi hai (normal word)                                      | \"works\", \"at\"                                                     |\n",
    "| **B-PER**  | Begin - Person         | Person entity ka pehla word (naam ki shuruaat)                         | \"John\" → `B-PER`                                                  |\n",
    "| **I-PER**  | Inside - Person        | Person entity ka continuation (naam ke dusre words)                    | \"Mary Jane\" → `Mary = B-PER`, `Jane = I-PER`                      |\n",
    "| **B-ORG**  | Begin - Organization   | Organization/company ka pehla word                                     | \"Google\" → `B-ORG`                                                |\n",
    "| **I-ORG**  | Inside - Organization  | Organization ke naam ke baaki words                                    | \"New York Times\" → `New = B-ORG`, `York = I-ORG`, `Times = I-ORG` |\n",
    "| **B-LOC**  | Begin - Location       | Location/place ka pehla word                                           | \"London\" → `B-LOC`                                                |\n",
    "| **I-LOC**  | Inside - Location      | Location ke naam ke baaki words                                        | \"New York\" → `New = B-LOC`, `York = I-LOC`                        |\n",
    "| **B-MISC** | Begin - Miscellaneous  | Miscellaneous entity ka pehla word (event, product, nationality, etc.) | \"Indian\" (nationality) → `B-MISC`                                 |\n",
    "| **I-MISC** | Inside - Miscellaneous | Miscellaneous entity ke dusre words (agar multi-word hai)              | \"South Korean\" → `South = B-MISC`, `Korean = I-MISC`              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVk5Jj_cHNXr"
   },
   "source": [
    "B → Begin (entity ka starting word)\n",
    "\n",
    "I → Inside (entity ke continuation wale words)\n",
    "\n",
    "O → Outside (koi entity nahi hai, normal word)\n",
    "\n",
    "WikiAnn (Wikipedia + Annotation) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxAnGW1qKspV"
   },
   "source": [
    "Complete Flow:\n",
    "\n",
    "\n",
    "\n",
    "1.   Hugging Face se WikiAnn English dataset load hota hai.\n",
    "2.   Randomly training (1000) aur test (250) samples select hote hain.\n",
    "3.   Tokens aur unke NER tags alag lists me nikale jate hain.\n",
    "4.   Return hote hain taaki training pipeline me use ho sake.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84WYdrwaT1Mv"
   },
   "outputs": [],
   "source": [
    "class BERTNERClassifier:\n",
    "    \"\"\"BERT for Named Entity Recognition\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', num_labels=9, max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "        self.model = BertForTokenClassification.from_pretrained(\n",
    "            model_name, num_labels=num_labels\n",
    "        )\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Label mapping\n",
    "        self.labels = ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG',\n",
    "                       'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
    "\n",
    "    def load_wikiann_data(self, sample_size=1000):\n",
    "        \"\"\"Load WikiAnn (Wikipedia + Annotation) dataset\"\"\"\n",
    "        print(\"Loading wikiann NER dataset...\")\n",
    "        dataset = load_dataset(\"wikiann\", \"en\")\n",
    "\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                       min(sample_size, len(dataset['train'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        test_indices = np.random.choice(len(dataset['test']),\n",
    "                                      min(sample_size//4, len(dataset['test'])),\n",
    "                                      replace=False)\n",
    "\n",
    "        # Convert numpy.int64 to int\n",
    "        train_tokens = [dataset['train'][int(i)]['tokens'] for i in train_indices]\n",
    "        train_labels = [dataset['train'][int(i)]['ner_tags'] for i in train_indices]\n",
    "\n",
    "        test_tokens = [dataset['test'][int(i)]['tokens'] for i in test_indices]\n",
    "        test_labels = [dataset['test'][int(i)]['ner_tags'] for i in test_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_tokens)}\")\n",
    "        print(f\"Test samples: {len(test_tokens)}\")\n",
    "\n",
    "        return train_tokens, train_labels, test_tokens, test_labels\n",
    "\n",
    "    def train(self, train_tokens, train_labels, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train the NER model\"\"\"\n",
    "        train_dataset = NERDataset(\n",
    "            train_tokens, train_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def evaluate(self, test_tokens, test_labels, batch_size=8):\n",
    "        \"\"\"Evaluate the NER model\"\"\"\n",
    "        test_dataset = NERDataset(\n",
    "            test_tokens, test_labels, self.tokenizer, self.max_length\n",
    "        )\n",
    "        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()\n",
    "                labels = labels.cpu().numpy()\n",
    "\n",
    "                # Collect valid predictions (ignore -100)\n",
    "                for i in range(preds.shape[0]):\n",
    "                    for j in range(preds.shape[1]):\n",
    "                        if labels[i][j] != -100:\n",
    "                            predictions.append(preds[i][j])\n",
    "                            true_labels.append(labels[i][j])\n",
    "\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "\n",
    "        return accuracy, f1\n",
    "\n",
    "    def predict(self, tokens_list):\n",
    "        \"\"\"Predict NER tags for new tokens\"\"\"\n",
    "        predictions = []\n",
    "\n",
    "        self.model.eval()\n",
    "        for tokens in tokens_list:\n",
    "            encoding = self.tokenizer(\n",
    "                tokens,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt',\n",
    "                is_split_into_words=True\n",
    "            )\n",
    "\n",
    "            input_ids = encoding['input_ids'].to(device)\n",
    "            attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=2).cpu().numpy()[0]\n",
    "\n",
    "                # Get alignment for original words\n",
    "                word_ids = encoding.word_ids(batch_index=0)\n",
    "                token_predictions = []\n",
    "                previous_word_idx = None\n",
    "\n",
    "                for i, word_idx in enumerate(word_ids):\n",
    "                    if word_idx is not None and word_idx != previous_word_idx:\n",
    "                        if word_idx < len(tokens):\n",
    "                            token_predictions.append(self.labels[preds[i]])\n",
    "                    previous_word_idx = word_idx\n",
    "\n",
    "                predictions.append(token_predictions)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noGChIBBUvKx"
   },
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"Dataset for Question Answering (SQuAD-style)\"\"\"\n",
    "\n",
    "    def __init__(self, questions, contexts, answers, tokenizer, max_length=512):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question = self.questions[idx]\n",
    "        context = self.contexts[idx]\n",
    "        answer = self.answers[idx]  # dict: {'text': [...], 'answer_start': [...]}\n",
    "\n",
    "        # Encode inputs with offsets to locate answer\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_offsets_mapping=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        offset_mapping = encoding.pop(\"offset_mapping\")[0]  # (max_length, 2)\n",
    "        start_positions = torch.tensor(0, dtype=torch.long)\n",
    "        end_positions = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        if answer and 'answer_start' in answer and answer['answer_start']:\n",
    "            answer_start = answer['answer_start'][0]\n",
    "            answer_text = answer['text'][0]\n",
    "            answer_end = answer_start + len(answer_text)\n",
    "\n",
    "            # Find token start/end matching answer char positions\n",
    "            for idx, (start, end) in enumerate(offset_mapping):\n",
    "                if start <= answer_start < end:\n",
    "                    start_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                if start < answer_end <= end:\n",
    "                    end_positions = torch.tensor(idx, dtype=torch.long)\n",
    "                    break\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),      # (max_length,)\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'start_positions': start_positions,\n",
    "            'end_positions': end_positions\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXlml3TfVDVH"
   },
   "outputs": [],
   "source": [
    "class BERTQuestionAnswering:\n",
    "    \"\"\"BERT for Question Answering (SQuAD)\"\"\"\n",
    "\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "        self.model = BertForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.model.to(device)\n",
    "\n",
    "    def load_squad_data(self, sample_size=2000):\n",
    "        \"\"\"Load and sample SQuAD dataset\"\"\"\n",
    "        print(\"Loading SQuAD dataset...\")\n",
    "        dataset = load_dataset(\"squad\")\n",
    "\n",
    "        train_indices = np.random.choice(len(dataset['train']),\n",
    "                                         min(sample_size, len(dataset['train'])),\n",
    "                                         replace=False)\n",
    "        val_indices = np.random.choice(len(dataset['validation']),\n",
    "                                       min(sample_size//4, len(dataset['validation'])),\n",
    "                                       replace=False)\n",
    "\n",
    "        train_questions = [dataset['train'][int(i)]['question'] for i in train_indices]\n",
    "        train_contexts = [dataset['train'][int(i)]['context'] for i in train_indices]\n",
    "        train_answers = [dataset['train'][int(i)]['answers'] for i in train_indices]\n",
    "\n",
    "        val_questions = [dataset['validation'][int(i)]['question'] for i in val_indices]\n",
    "        val_contexts = [dataset['validation'][int(i)]['context'] for i in val_indices]\n",
    "        val_answers = [dataset['validation'][int(i)]['answers'] for i in val_indices]\n",
    "\n",
    "        print(f\"Train samples: {len(train_questions)}\")\n",
    "        print(f\"Validation samples: {len(val_questions)}\")\n",
    "\n",
    "        return (train_questions, train_contexts, train_answers,\n",
    "                val_questions, val_contexts, val_answers)\n",
    "\n",
    "    def train(self, questions, contexts, answers, epochs=1, batch_size=8, learning_rate=2e-5):\n",
    "        \"\"\"Train QA model using QADataset (offset mapping based)\"\"\"\n",
    "        train_dataset = QADataset(questions, contexts, answers, self.tokenizer, self.max_length)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "        total_steps = len(train_loader) * epochs\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=total_steps//10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "\n",
    "            for batch in progress_bar:\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                start_positions = batch['start_positions'].to(device)\n",
    "                end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    start_positions=start_positions,\n",
    "                    end_positions=end_positions\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})\n",
    "\n",
    "            print(f'Epoch {epoch+1}, Average Loss: {total_loss/len(train_loader):.4f}')\n",
    "\n",
    "    def answer_question(self, question, context, max_answer_len=30):\n",
    "        \"\"\"Answer a single question given context\"\"\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            question,\n",
    "            context,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            start_logits = outputs.start_logits\n",
    "\n",
    "            end_logits = outputs.end_logits\n",
    "\n",
    "            start_idx = torch.argmax(start_logits, dim=1).item()\n",
    "\n",
    "            end_idx = torch.argmax(end_logits, dim=1).item()\n",
    "\n",
    "            # Ensure valid span\n",
    "            if end_idx < start_idx:\n",
    "                end_idx = start_idx\n",
    "\n",
    "            if (end_idx - start_idx) > max_answer_len:\n",
    "                end_idx = start_idx + max_answer_len\n",
    "\n",
    "            # Decode predicted tokens\n",
    "            answer_tokens = input_ids[0][start_idx:end_idx+1]\n",
    "\n",
    "            answer = self.tokenizer.decode(answer_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "            return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YBxYZ0B7VTNX"
   },
   "outputs": [],
   "source": [
    "def run_text_classification_demo():\n",
    "\n",
    "    \"\"\"Demo for text classification\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TEXT CLASSIFICATION (Sentiment Analysis) DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    classifier = BERTTextClassifier(num_classes=2)\n",
    "\n",
    "    # Load data\n",
    "    train_texts, train_labels, test_texts, test_labels = classifier.load_imdb_data(sample_size=1000)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample Review: {train_texts[0][:200]}...\")\n",
    "    print(f\"Label: {'Positive' if train_labels[0] == 1 else 'Negative'}\")\n",
    "\n",
    "    # Train for 2 epochs (small for demo)\n",
    "    classifier.train(train_texts, train_labels, epochs=1, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1, report = classifier.evaluate(test_texts, test_labels, batch_size=8)\n",
    "\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Test custom examples\n",
    "    custom_reviews = [\n",
    "        \"This movie was fantastic! Amazing acting and great plot.\",\n",
    "        \"Boring and terrible. Waste of time.\",\n",
    "        \"Not bad, could be better though.\"\n",
    "    ]\n",
    "\n",
    "    predictions, probabilities = classifier.predict(custom_reviews)\n",
    "\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "\n",
    "    for text, pred, prob in zip(custom_reviews, predictions, probabilities):\n",
    "\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "\n",
    "        confidence = prob[pred] * 100\n",
    "\n",
    "        print(f\"'{text[:50]}...' -> {sentiment} ({confidence:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ3FV7p3Vo7G"
   },
   "outputs": [],
   "source": [
    "def run_ner_demo():\n",
    "\n",
    "    \"\"\"Demo for Named Entity Recognition\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"NAMED ENTITY RECOGNITION DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    ner_model = BERTNERClassifier(num_labels=9)\n",
    "\n",
    "    # Load small subset for demo (fast training)\n",
    "    train_tokens, train_labels, test_tokens, test_labels = ner_model.load_wikiann_data(sample_size=500)\n",
    "\n",
    "    # Show sample\n",
    "    print(f\"\\nSample tokens: {train_tokens[0][:10]}\")\n",
    "\n",
    "    label_names = [ner_model.labels[l] if l < len(ner_model.labels) else \"O\" for l in train_labels[0][:10]]\n",
    "\n",
    "    print(f\"Sample labels: {label_names}\")\n",
    "\n",
    "    # Train for 2 epochs\n",
    "    ner_model.train(train_tokens, train_labels, epochs=1, batch_size=8)\n",
    "\n",
    "    # Evaluate\n",
    "    accuracy, f1 = ner_model.evaluate(test_tokens, test_labels, batch_size=8)\n",
    "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Custom examples\n",
    "    custom_sentences = [\n",
    "        [\"John\", \"Smith\", \"works\", \"at\", \"Google\", \"in\", \"California\"],\n",
    "        [\"Apple\", \"Inc.\", \"was\", \"founded\", \"by\", \"Steve\", \"Jobs\"]\n",
    "    ]\n",
    "\n",
    "    predictions = ner_model.predict(custom_sentences)\n",
    "    print(f\"\\nCustom Predictions:\")\n",
    "    for tokens, preds in zip(custom_sentences, predictions):\n",
    "        print(\"Tokens:\", tokens)\n",
    "        print(\"Labels:\", preds)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8Fx5D5uVpki"
   },
   "outputs": [],
   "source": [
    "def run_qa_demo():\n",
    "\n",
    "    \"\"\"Demo for Question Answering\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"QUESTION ANSWERING DEMO\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    qa_model = BERTQuestionAnswering()\n",
    "\n",
    "    # Load small subset (for speed)\n",
    "    (train_questions, train_contexts, train_answers,\n",
    "     val_questions, val_contexts, val_answers) = qa_model.load_squad_data(sample_size=500)\n",
    "\n",
    "    # Safe sample print\n",
    "    ans_text = train_answers[0]['text'][0] if train_answers[0]['text'] else 'No answer'\n",
    "\n",
    "    print(f\"\\nSample Question: {train_questions[0]}\")\n",
    "\n",
    "    print(f\"Sample Context: {train_contexts[0][:200]}...\")\n",
    "\n",
    "    print(f\"Sample Answer: {ans_text}\")\n",
    "\n",
    "    # Train model (2 epochs for demo)\n",
    "    qa_model.train(train_questions, train_contexts, train_answers, epochs=1, batch_size=4)\n",
    "\n",
    "    # Test on custom questions\n",
    "    print(f\"\\nCustom Q&A Examples:\")\n",
    "\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is the capital of France?\",\n",
    "            \"context\": \"France is a country in Europe. Paris is the capital and largest city of France. The city is known for the Eiffel Tower and the Louvre Museum.\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Who founded Apple?\",\n",
    "            \"context\": \"Apple Inc. is an American technology company. It was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in 1976. The company is known for products like iPhone and Mac.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    for case in test_cases:\n",
    "        answer = qa_model.answer_question(case[\"question\"], case[\"context\"], max_answer_len=30)\n",
    "        print(f\"Q: {case['question']}\")\n",
    "        print(f\"A: {answer}\")\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "q1s2cTD6VzOe",
    "outputId": "23ec3195-9aac-4370-cf0e-8eb55f99a9cb"
   },
   "outputs": [],
   "source": [
    "print(\"BERT Multi-Task Demo\")\n",
    "print(\"Choose a task to run:\")\n",
    "print(\"1. Text Classification (Sentiment Analysis)\")\n",
    "print(\"2. Named Entity Recognition (NER)\")\n",
    "print(\"3. Question Answering\")\n",
    "print(\"4. Run All Tasks\")\n",
    "\n",
    "choice = input(\"\\nEnter your choice (1-4): \").strip()\n",
    "\n",
    "try:\n",
    "    if choice == \"1\":\n",
    "        run_text_classification_demo()\n",
    "    elif choice == \"2\":\n",
    "        run_ner_demo()\n",
    "    elif choice == \"3\":\n",
    "        run_qa_demo()\n",
    "    elif choice == \"4\":\n",
    "        run_text_classification_demo()\n",
    "        run_ner_demo()\n",
    "        run_qa_demo()\n",
    "    else:\n",
    "        print(\"Invalid choice! Please run again.\")\n",
    "except Exception as e:\n",
    "    print(\"\\n--- ERROR OCCURRED ---\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nMake sure you have:\")\n",
    "    print(\"1. Installed required packages:\")\n",
    "    print(\"   pip install torch transformers datasets scikit-learn tqdm numpy\")\n",
    "    print(\"2. Loaded all classes & dataset helpers (BERTTextClassifier, BERTNERClassifier, BERTQuestionAnswering, TextClassificationDataset, NERDataset, QADataset)\")\n",
    "    print(\"3. Using the fixed versions (with int casting, offset_mapping for QA, word_ids fix for NER)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfXDa53caeZO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
